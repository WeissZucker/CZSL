{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNMEBGuOtd5z"
   },
   "source": [
    "# MIT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gm9YyejSBahW"
   },
   "outputs": [],
   "source": [
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random, io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONFLpL2v1Z8S"
   },
   "outputs": [],
   "source": [
    "from symnet.utils import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RtyStmYy05Cb",
    "outputId": "acc57faf-cca3-4b8c-de91-3b84f64347fa"
   },
   "outputs": [],
   "source": [
    "# train [img, attr_id, obj_id, pair_id, img_feature, img, attr_id, obj_id, pair_id, img_feature, aff_mask]\n",
    "# test [img, attr_id, obj_id, pair_id, img_feature, aff_mask]\n",
    "\n",
    "train_dataloader = dataset.get_dataloader('MIT', 'train', batchsize=64, with_image=True, shuffle=True)\n",
    "test_dataloader = dataset.get_dataloader('MIT', 'test', batchsize=64, with_image=True)\n",
    "\n",
    "obj_class = len(train_dataloader.dataset.obj2idx.keys())\n",
    "attr_class = len(train_dataloader.dataset.attr2idx.keys())\n",
    "\n",
    "print(f\"obj_class: {obj_class}, attr_class: {attr_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRkyCls9tnKp"
   },
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sArcE1du8P2"
   },
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, in_features, out_features):\n",
    "    super(MLP, self).__init__()\n",
    "\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(in_features, in_features),\n",
    "        nn.BatchNorm1d(in_features),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(in_features, out_features))\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.mlp(x)\n",
    "\n",
    "\n",
    "class MLP2(nn.Module):\n",
    "  def __init__(self, in_features, out_features):\n",
    "    super(MLP2, self).__init__()\n",
    "\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(in_features, in_features//2),\n",
    "        nn.BatchNorm1d(in_features//2),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(in_features//2, in_features//4),\n",
    "        nn.BatchNorm1d(in_features//4),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(in_features//4, out_features))\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.mlp(x)\n",
    "\n",
    "\n",
    "class MLP3(nn.Module):\n",
    "  def __init__(self, in_features, out_features):\n",
    "    super(MLP3, self).__init__()\n",
    "\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(in_features, 1400),\n",
    "        nn.BatchNorm1d(1400),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(1400, 800),\n",
    "        nn.BatchNorm1d(800),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(800, 400),\n",
    "        nn.BatchNorm1d(400),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(400, out_features))\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.mlp(x)\n",
    "\n",
    "\n",
    "class HalvingMLP(nn.Module):\n",
    "  def __init__(self, in_features, out_features, num_layers=None):\n",
    "      super(HalvingMLP, self).__init__()\n",
    "      layers = []\n",
    "      for i in range(num_layers):\n",
    "        layer = nn.Sequential(\n",
    "          nn.Linear(in_features, in_features//2),\n",
    "          nn.BatchNorm1d(in_features//2),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout())\n",
    "        layers.append(layer)\n",
    "        in_features //= 2\n",
    "      layers.append(nn.Linear(in_features, out_features))\n",
    "      self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.mlp(x)\n",
    "\n",
    "def frozen(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "class CompoResnet(nn.Module):\n",
    "  def __init__(self, resnet, obj_class, attr_class, MLP=MLP):\n",
    "    super(CompoResnet, self).__init__()\n",
    "    in_features = resnet.fc.in_features # 2048 for resnet101\n",
    "    resnet.fc = Identity()\n",
    "    self.resnet = resnet\n",
    "    self.obj_fc = MLP(in_features, obj_class)\n",
    "    self.attr_fc = MLP(in_features, attr_class)\n",
    "\n",
    "  def forward(self, x):\n",
    "    img_features = self.resnet(x)\n",
    "    obj_pred = self.obj_fc(img_features)\n",
    "    attr_pred = self.attr_fc(img_features)\n",
    "    return obj_pred, attr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmBi7kDn0YaO"
   },
   "outputs": [],
   "source": [
    "def train_with_config(config, checkpoint_dir=None, num_epochs=1):\n",
    "  lr = config['lr']\n",
    "  resnet_name = config['resnet']\n",
    "  num_mlp_layers = config['num_mlp_layers']\n",
    "  mlp = partial(HalvingMLP, num_layers=num_mlp_layers)\n",
    "  batch_size = 64\n",
    "\n",
    "  resnet = frozen(torch.hub.load('pytorch/vision:v0.9.0', resnet_name, pretrained=True))\n",
    "  compoResnet = CompoResnet(resnet, obj_class, attr_class, mlp).to(dev)\n",
    "  obj_loss_history = [[],[]]\n",
    "  attr_loss_history = [[],[]]\n",
    "  optimizer = optim.Adam(compoResnet.parameters(), lr=lr)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  if checkpoint_dir:\n",
    "    model_state, optimizer_state, obj_loss_history, attr_loss_history = torch.load(\n",
    "        os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "    compoResnet.load_state_dict(model_state)\n",
    "    optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "\n",
    "  train(compoResnet, optimizer, criterion, num_epochs, obj_loss_history, attr_loss_history, batch_size, use_tune=True)\n",
    "\n",
    "def train(net, optimizer, criterion, num_epochs, obj_loss_history, attr_loss_history, batch_size, curr_epoch=0, use_tune=False, model_dir=None):\n",
    "  dset = dataset.get_dataloader('MIT', 'train', with_image=True).dataset\n",
    "  test_abs = int(len(dset) * 0.8)\n",
    "  train_subset, val_subset = random_split(\n",
    "        dset, [test_abs, len(dset) - test_abs])\n",
    "  train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "  val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  for epoch in range(curr_epoch, curr_epoch+num_epochs):\n",
    "    epoch_steps = 0\n",
    "    obj_running_loss = 0.0\n",
    "    attr_running_loss = 0.0\n",
    "    net.train()\n",
    "    for i, batch in tqdm.tqdm(\n",
    "        enumerate(train_dataloader),\n",
    "        total=len(train_dataloader),\n",
    "        disable=use_tune,\n",
    "        position=0,\n",
    "        leave=True,\n",
    "        postfix='Train: epoch %d/%d'%(epoch, num_epochs)):\n",
    "      optimizer.zero_grad()\n",
    "      img, attr_id, obj_id = batch[:3]\n",
    "      if len(img) == 1:\n",
    "        # Batchnorm doesn't accept batch with size 1\n",
    "        continue\n",
    "      obj_pred, attr_pred = net(img.to(dev))\n",
    "      obj_loss = criterion(obj_pred, obj_id.to(dev))\n",
    "      attr_loss = criterion(attr_pred, attr_id.to(dev))\n",
    "      loss = obj_loss + attr_loss\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      obj_running_loss += obj_loss.item()\n",
    "      attr_running_loss += attr_loss.item()\n",
    "      epoch_steps += 1\n",
    "      if i % 100 == 99:\n",
    "          print(\"[%d, %5d] obj_loss: %.3f, attr_loss: %.3f\" % (epoch+1, i + 1,\n",
    "                                          obj_running_loss / epoch_steps, attr_running_loss / epoch_steps))\n",
    "          obj_loss_history[0].append(obj_running_loss/epoch_steps)\n",
    "          attr_loss_history[0].append(attr_running_loss/epoch_steps)\n",
    "          running_loss = 0.0\n",
    "\n",
    "    # Validation loss\n",
    "    obj_val_loss = 0.0\n",
    "    attr_val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    \n",
    "    net.eval()\n",
    "    for i, batch in tqdm.tqdm(\n",
    "          enumerate(val_dataloader),\n",
    "          total=len(val_dataloader),\n",
    "          disable=use_tune,\n",
    "          position=0,\n",
    "          leave=True):\n",
    "        with torch.no_grad():\n",
    "            img, attr_id, obj_id = batch[:3]\n",
    "            obj_pred, attr_pred = net(img.to(dev))\n",
    "            obj_loss = criterion(obj_pred, obj_id.to(dev))\n",
    "            attr_loss = criterion(attr_pred, attr_id.to(dev))\n",
    "            obj_val_loss += obj_loss.cpu().numpy()\n",
    "            attr_val_loss += attr_loss.cpu().numpy()\n",
    "            val_steps += 1\n",
    "    \n",
    "    obj_val_loss /= val_steps\n",
    "    attr_val_loss /= val_steps\n",
    "    print(\"[%d] obj_val_loss: %.3f, attr_val_loss: %.3f\" % (epoch+1, obj_avl_loss, attr_val_loss ))\n",
    "    obj_loss_history[1].append(obj_avl_loss)\n",
    "    attr_loss_history[1].append(attr_val_loss)\n",
    "        \n",
    "    if use_tune:\n",
    "      with tune.checkpoint_dir(epoch) as checkpoint_dir: \n",
    "          path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "          torch.save({\n",
    "                      'model_state_dict': net.state_dict(),\n",
    "                      'optimizer_state_dict': optimizer.state_dict(),\n",
    "                      'obj_loss': obj_loss_history,\n",
    "                      'attr_loss': attr_loss_history,\n",
    "                      }, path)\n",
    "      acc = calc_acc(net, val_dataloader, use_tune)\n",
    "      tune.report(loss=(obj_val_loss+attr_val_loss), accuracy=acc)\n",
    "      print(\"accuracy: \", acc)\n",
    "    else:\n",
    "      if model_dir:\n",
    "        model_path = os.path.join(model_dir, f\"model_{curr_epoch+epoch}.pt\")\n",
    "        torch.save({\n",
    "                      'model_state_dict': net.state_dict(),\n",
    "                      'optimizer_state_dict': optimizer.state_dict(),\n",
    "                      'obj_loss': obj_loss_history,\n",
    "                      'attr_loss': attr_loss_history,\n",
    "                      }, model_path)\n",
    "    print(\"Finished training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wXuNIx6QRuZ"
   },
   "outputs": [],
   "source": [
    "def calc_acc(model, test_dataloader, use_tune=False):  \n",
    "  def match(labels, preds):\n",
    "    preds = torch.argmax(preds, axis=-1)\n",
    "    return torch.sum(preds == labels)\n",
    "\n",
    "  def compoMatch(obj_labels, obj_preds, attr_labels, attr_preds):\n",
    "    obj_preds = torch.argmax(obj_preds, axis=-1)\n",
    "    attr_preds = torch.argmax(attr_preds, axis=-1)\n",
    "    comp_match = (obj_labels == obj_preds) * (attr_labels == attr_preds)\n",
    "    return torch.sum(comp_match)\n",
    "\n",
    "  obj_match, attr_match, comp_match = 0, 0, 0\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i, batch in tqdm.tqdm(\n",
    "        enumerate(test_dataloader),\n",
    "        total=len(test_dataloader),\n",
    "        disable=use_tune,\n",
    "        position=0,\n",
    "        leave=True):\n",
    "      img, attr_id, obj_id = batch[:3]\n",
    "      obj_preds, attr_preds = model(img.to(dev))\n",
    "      obj_preds, attr_preds = obj_preds.to('cpu'), attr_preds.to('cpu')\n",
    "      obj_match += match(obj_id, obj_preds)\n",
    "      attr_match += match(attr_id, attr_preds)\n",
    "      comp_match += compoMatch(obj_id, obj_preds, attr_id, attr_preds)\n",
    "  return np.array([obj_match, attr_match, comp_match]) / len(test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = frozen(torch.hub.load('pytorch/vision:v0.9.0', 'resnet101', pretrained=True))\n",
    "mlp = partial(HalvingMLP, num_layers=2)\n",
    "compoResnet = CompoResnet(resnet, obj_class, attr_class, mlp).to(dev)\n",
    "\n",
    "obj_loss_history = [[],[]]\n",
    "attr_loss_history = [[],[]]\n",
    "optimizer = torch.optim.Adam(compoResnet.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "curr_epoch = 0\n",
    "\n",
    "model_dir = './models/'\n",
    "model_name = None\n",
    "model_path = None if not model_name else os.path.join(model_dir, model_name)\n",
    "\n",
    "if model_path:\n",
    "  #checkpoint = torch.load(model_path), map_location=torch.device('cpu'))\n",
    "  checkpoint = torch.load(model_path)\n",
    "  compoResnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  curr_epoch = checkpoint['epoch']\n",
    "  obj_loss_history = checkpoint['obj_loss']\n",
    "  attr_loss_history = checkpoint['attr_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(compoResnet, optimizer, criterion, 1, obj_loss_history, attr_loss_history, 10, curr_epoch=0, use_tune=False, model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQfa8FFXGZd4"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"resnet\": tune.choice(['resnet18', 'resnet50', 'resnet101']),\n",
    "    \"num_mlp_layers\": tune.choice([1,2,4,6]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jcs6yz64H1-K",
    "outputId": "36edb0ab-b5db-4da4-bd59-37adeab1ad26"
   },
   "outputs": [],
   "source": [
    "num_samples = 12\n",
    "num_epochs = 6\n",
    "scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "reporter = CLIReporter(metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "result = tune.run(\n",
    "    partial(train_with_config, num_epochs=num_epochs),\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": 0.32},\n",
    "    config=config,\n",
    "    num_samples=num_samples,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TwtaoRxPuqT"
   },
   "outputs": [],
   "source": [
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final validation loss: {}\".format(\n",
    "    best_trial.last_result[\"loss\"]))\n",
    "print(\"Best trial final validation accuracy: {}\".format(\n",
    "    best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "resnet = frozen(torch.hub.load('pytorch/vision:v0.9.0', best_trial.config[\"resnet\"], pretrained=True))\n",
    "best_mlp = partial(HalvingMLP, num_layers=best_trial.config[\"num_mlp_layers\"])\n",
    "best_trained_model = CompoResnet(resnet, obj_class, attr_class, best_mlp).to(dev)\n",
    "\n",
    "best_checkpoint_dir = best_trial.checkpoint.value\n",
    "model_state = torch.load(os.path.join(\n",
    "    best_checkpoint_dir, \"checkpoint\"))['model_state_dict']\n",
    "best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "test_acc = calc_acc(best_trained_model, test_dataloader)\n",
    "print(\"\\nBest trial test set accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPjvKyZfNL_n"
   },
   "source": [
    "Matches:\n",
    "\n",
    "[0.30456985, 0.15528112, 0.02720025] : MLP2, 30 Epochs, Adam"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zUAbnBhatXQT",
    "mNMEBGuOtd5z"
   ],
   "name": "mitstates_tune.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
